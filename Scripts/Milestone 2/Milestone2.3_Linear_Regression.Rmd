---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# "mitoxantrone" ist okay
# afatinib ist okay 

#entweder zardaverine oder romidepsin

```

```{r, echo=FALSE}
# subsetting datasets in order to obtain ovary cancer cell line data 
ovary_achilles <- prism.achilles[rownames(prism.achilles) %in% vector_ovary_DepMI,]
B_achilles = as.matrix(ovary_achilles)
B_achilles_new <- B_achilles[ , which(apply(B_achilles, 2, var) != 0)] ## removing zero variance values
pca_achilles = prcomp(B_achilles_new, scale = TRUE)


ovary_exp <- prism.exp[rownames(prism.exp) %in% vector_ovary_DepMI,]
B_exp = as.matrix(ovary_exp)
B_exp_new <- B_exp[ , which(apply(B_exp, 2, var) != 0)] ## removing zero variance values
pca_exp = prcomp(B_exp_new, scale = TRUE)
```





```{r}



regression_data <- prism_reduced_ovary_no_NAs["disulfiram"]

regression_data$pca_achilles1 = pca_achilles$x[match(rownames(regression_data), names(pca_achilles$x[,1])),1]
regression_data$pca_achilles2 = pca_achilles$x[match(rownames(regression_data), names(pca_achilles$x[,2])),2]
regression_data$pca_achilles3 = pca_achilles$x[match(rownames(regression_data), names(pca_achilles$x[,3])),3]
regression_data$pca_exp1 = pca_exp$x[match(rownames(regression_data), names(pca_exp$x[,1])),1]
regression_data$pca_exp2 = pca_exp$x[match(rownames(regression_data), names(pca_exp$x[,2])),2]
regression_data$pca_cnv1 = pca_cnv$x[match(rownames(regression_data), names(pca_cnv$x[,1])),1]
regression_data$pca_cnv2 = pca_cnv$x[match(rownames(regression_data), names(pca_cnv$x[,2])),2]

#regression_data$ic = moa_drugs_ovary$`CR8-(R)`

regression_data_no_nas <- na.omit(regression_data)

pairs(regression_data_no_nas[,-1], pch=20, col="Lightblue3")

```


```{r}
learning_celllines = sample(1:nrow(regression_data_no_nas), 13, replace = F)
learning_regression <- regression_data_no_nas[learning_celllines,]
check_regression <- regression_data_no_nas[-learning_celllines,]
```


```{r}
regression_model <- lm(formula = (disulfiram) ~  .,data=learning_regression)
summary(regression_model)
```


```{r}
mean(regression_model$residuals)

qqnorm(regression_model$residuals, ylab = "Residuals", main = "Q-Q Plot residuals" );qqline(regression_model$residuals)
```

```{r}


cor(learning_regression$disulfiram, regression_model$residuals)

plot(learning_regression$disulfiram, regression_model$fitted.values,pch=20, col="light blue", xlab = "Control", ylab = "Residuals");abline(0,1,col="red")


# the residuals should have the expected value of zero 
```


```{r}
proliferation_pred <- predict(regression_model, newdata = check_regression)

plot(check_regression$disulfiram, proliferation_pred,pch=20, col="lightblue3", xlab = "Real values", ylab = "Predicted values");abline(0,1,col="red" )
title("Predicted values from the regression to real values")
```


```{r}
#RMSE of our residuals
n = nrow(learning_regression)
rmse.learning = sqrt(1/n*sum(regression_model$residuals^2))

#RMSE of predicted model
n=nrow(check_regression)
# the residuals of the predicted model are: 
residuals = check_regression$disulfiram- proliferation_pred
rmse.check = sqrt(1/n*sum(residuals^2))

rmse.learning
rmse.check
```

- Multiple Regression = looking for an equation that describes the linear relationship between a dependent variable and **multiple** independent variables 
- the difference between each observed value and each estimated value is called "residual" and can be calculated for each observation 
- lm (dependent variable ~ data set used):
**Call:**
- the specification of the calculated model is specified --> the command, the dependent and independent variables as well as the dataset 
- residuals: quick overview of the size and distribution of the residuals; the minimum, 1st quantile, median and 3rd quantile and the maximum (residuals should be normally distributed i.e. the median should be approximately zero + the model is better the smaller the residuals are)
- F-statistic is our test statistic; we can take the value of the F-statistic and compare it with the value of the F-distribution table or we can look at the p-value
--> the null hypothesis of regression model is: the model does not explain the dependent variable; the corresponding alternative hypothesis is: the model explains the dependent variable
--> if the p-value is small, we reject null hypothesis and proceed with alternative hypothesis 
- significance level alpha = 0.01 is definied in advance of analysis and p value would need to be smaller than 1 % for us to assume that the model explains the dependent variable
- R^2 (multiple R-squared) shows how much variation of the dependent variable is explained by the model e.g if it's 0.2, 20 % of the variation is explained by the model and 80 % remain unexplained
- adjusted-R^2 is better the higher it is 
- standard error of the residuals is a measure for the quality of the regression model (can be interpreted as the average deviation of the estimated values from the true values [value is displayed in the unit of the dependent variable])
- Coefficients: a matrix and we get the coefficients of the regression model (Estimate), the standard error (Std. Error), the t-value and the p-value (Pr(>|t|)). We have a null hypothesis and an alternative hypothesis for each variable. The null hypothesis is: The variable does not contribute to the explanation of the dependent variable. The alternative hypothesis is: The variable contributes to the explanation of the dependent variable; The t-value is our test statistic and the Pr(>|t|) is used to reject the null hypothesis or not
--> if the p-value is really small, we can reject the null hypothesis for each specified level of significance and assume that the variables contribute to the explanation of the model 





